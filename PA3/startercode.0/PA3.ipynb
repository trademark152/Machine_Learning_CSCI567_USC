{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #3 Programming Assignment\n",
    "CSCI567, Spring 2019<br>Victor Adamchik<br>**Due: 11:59 pm, March 3rd 2019**\n",
    "\n",
    "\n",
    "### Before you start: \n",
    "On Vocareum, when you submit your homework, it takes around 5-6 minutes to run the grading scripts and evaluate your code. So, please be patient regarding the same.<br>\n",
    "\n",
    "\n",
    "## Office Hour for Project Assignment 3\n",
    "Office hours for Anirudh: <br>\n",
    "February 15th, 2pm - 3pm<br>\n",
    "February 22nd, 2pm - 3pm<br>\n",
    "March 1st, 2pm - 4pm<br>\n",
    "<br>\n",
    "Office hours for Piyush:<br>\n",
    "February 13th, 2pm - 3pm<br>\n",
    "February 21nd, 2pm - 3pm<br>\n",
    "March 4th, 2pm - 4pm<br>\n",
    "<br>\n",
    "Also, you can post your question on Piazza under pa-3 folder. We will try our best to answer all questions as soon as possible. Please make sure you read previous posts before creating a new post in case your question has been answered before. However, if you have any urgent issue, please feel free to send an email to both of us. <br>\n",
    "Anirudh, Kashi: kashia@usc.edu<br>\n",
    "Piyush, Umate: pumate@usc.edu<br>\n",
    "\n",
    "\n",
    "\n",
    "## Problem 1 Neural Networks (40 points)\n",
    "![MLP_diagram.png](attachment:MLP_diagram.png)\n",
    "<br><br>\n",
    "For this Assignment, you are asked to implement neural networks. We will be using this neural network to classify MNIST database of handwritten digits (0-9). The architecture of the multi-layer perceptron (MLP, just another term for fully connected feedforward networks we discussed in the lecture) you will be implementing is shown in figure 1. Following MLP is designed for a K-class classification problem. \n",
    "\n",
    "Let $(x\\in\\mathbb{R}^D, y\\in\\{1,2,\\cdots,K\\})$ be a labeled instance, such an MLP performs the following computations.\n",
    "<br><br><br><br>\n",
    "$$\n",
    "\\begin{align}\n",
    " \\textbf{input features}: \\hspace{15pt} & x \\in \\mathbb{R}^D \\\\\n",
    " \\textbf{linear}^{(1)}: \\hspace{15pt} & u = W^{(1)}x + b^{(1)} \\hspace{2em}, W^{(1)} \\in \\mathbb{R}^{M\\times D} \\text{ and } b^{(1)} \\in \\mathbb{R}^{M}  \\label{linear_forward}\\\\\n",
    " \\textbf{tanh}:\\hspace{15pt} & h =\\cfrac{2}{1+e^{-2u}}-1 \\label{tanh_forward}\\\\\n",
    " \\textbf{relu}: \\hspace{15pt} & h = max\\{0, u\\} =\n",
    "\\begin{bmatrix}\n",
    "\\max\\{0, u_1\\}\\\\\n",
    "\\vdots \\\\\n",
    "\\max\\{0, u_M\\}\\\\\n",
    "\\end{bmatrix} \\label{relu_forward}\\\\\n",
    " \\textbf{linear}^{(2)}: \\hspace{15pt} & a = W^{(2)}h + b^{(2)} \\hspace{2em}, W^{(2)} \\in \\mathbb{R}^{K\\times M} \\text{ and } b^{(2)} \\in \\mathbb{R}^{K} \\label{linear2_forward}\\\\\n",
    " \\textbf{softmax}: \\hspace{15pt} & z = \\begin{bmatrix}\n",
    "\\cfrac{e^{a_1}}{\\sum_{k} e^{a_{k}}}\\\\\n",
    "\\vdots \\\\\n",
    "\\cfrac{e^{a_K}}{\\sum_{k} e^{a_{k}}} \\\\\n",
    "\\end{bmatrix}\\\\\n",
    " \\textbf{predicted label}: \\hspace{15pt} & \\hat{y} = argmax_k z_k.\n",
    "%& l = -\\sum_{k} y_{k}\\log{\\hat{y_{k}}} \\hspace{2em}, \\vy \\in \\mathbb{R}^{k} \\text{ and } y_k=1 \\text{ if } \\vx \\text{ belongs to the } k' \\text{-th class}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "For a $K$-class classification problem, one popular loss function for training (i.e., to learn $W^{(1)}$, $W^{(2)}$, $b^{(1)}$, $b^{(2)}$) is the cross-entropy loss.\n",
    "Specifically we denote the cross-entropy loss with respect to the training example $(x, y)$ by $l$:\n",
    "<br><br>\n",
    "$$\n",
    "\\begin{align}\n",
    "  l = -\\log (z_y) = \\log \\left( 1 + \\sum_{k\\neq y} e^{a_k - a_y} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "<br><br>\n",
    "Note that one should look at $l$ as a function of the parameters of the network, that is, $W^{(1)}, b^{(1)}, W^{(2)}$ and $b^{(2)}$.\n",
    "For ease of notation, let us define the one-hot (i.e., 1-of-$K$) encoding of a class $y$ as\n",
    "\n",
    "\\begin{align}\n",
    "y \\in \\mathbb{R}^K \\text{ and }\n",
    "y_k =\n",
    "\\begin{cases}\n",
    "1, \\text{ if }y = k,\\\\\n",
    "0, \\text{ otherwise}.\n",
    "\\end{cases} \n",
    "\\end{align}\n",
    "so that\n",
    "\\begin{align} \n",
    "l = -\\sum_{k} y_{k}\\log{z_k} = \n",
    "-y^T\n",
    "\\begin{bmatrix}\n",
    "\\log z_1\\\\\n",
    "\\vdots \\\\\n",
    "\\log z_K\\\\\n",
    "\\end{bmatrix}\n",
    "= -y^T\\log{z}.\n",
    "\\end{align}\n",
    "\n",
    "We can then perform error-backpropagation, a way to compute partial derivatives (or gradients) w.r.t the parameters of a neural network, and use gradient-based optimization to learn the parameters.  \n",
    "\n",
    "\n",
    "Submission: All you need to submit is neural_networks.py\n",
    "\n",
    "### Q1.1 Mini batch Gradient Descent \n",
    "First, You need to implement mini-batch gradient descent which is a gradient-based optimization to learn the parameters of the neural network. \n",
    "<br>\n",
    "$$\n",
    "\\begin{align}\n",
    "\\upsilon = \\alpha \\upsilon - \\eta \\delta_t\\\\\n",
    "w_t = w_{t-1} + \\upsilon\n",
    "\\end{align}\n",
    "$$\n",
    "<br>\n",
    "You can use the formula above to update the weights using momentum. <br>\n",
    "Here,\n",
    "$\\alpha$ is the discount factor such that $\\alpha \\in (0, 1)$ <br>\n",
    "$\\upsilon$ is the velocity update<br>\n",
    "$\\eta$ is the learning rate<br>\n",
    "$\\delta_t$ is the gradient<br>\n",
    "\n",
    "You need to handle with as well without momentum scenario in the ```miniBatchGradientDescent``` function.\n",
    "\n",
    "* ```TODO 1```\n",
    "You need to complete ```def miniBatchGradientDescent(model, momentum, _lambda, _alpha, _learning_rate)``` in ```neural_networks.py```\n",
    "\n",
    "### Q1.2 Linear Layer (10 points)\n",
    "Second, You need to implement the linear layer of MLP. In this part, you need to implement 3 python functions in ```class linear_layer```. In ```def __init__(self, input_D, output_D)``` function, you need to initialize W with random values using np.random.normal such that the mean is 0 and standard deviation is 0.1. You also need to initialize gradients to zeroes in the same function. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{forward pass:}\\hspace{2em} &\n",
    "u = \\text{linear}^{(1)}\\text{.forward}(x) = W^{(1)}x + b^{(1)},\\\\\n",
    "&\\text{where } W^{(1)} \\text{ and } b^{(1)} \\text{ are its parameters.}\\nonumber\\\\ \n",
    "\\nonumber\\\\\n",
    "\\text{backward pass:}\\hspace{2em} &[\\frac{\\partial l}{\\partial x}, \\frac{\\partial l}{\\partial W^{(1)}}, \\frac{\\partial l}{\\partial b^{(1)}}] = \\text{linear}^{(1)}\\text{.backward}(x, \\frac{\\partial l}{\\partial u}).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You can use the above formula as a reference to implement the ```def forward(self, X)``` forward pass and ```def backward(self, X, grad)``` backward pass in class linear_layer. In backward pass, you only need to return the backward_output. You also need to compute gradients of W and b in backward pass. \n",
    "\n",
    "* ```TODO 2```\n",
    "You need to complete ```def __init__(self, input_D, output_D)``` in ```class linear_layer``` of ```neural_networks.py```\n",
    "* ```TODO 3```\n",
    "You need to complete ```def forward(self, X)``` in ```class linear_layer``` of  ```neural_networks.py```\n",
    "* ```TODO 4```\n",
    "You need to complete ```def backward(self, X, grad)``` in ```class linear_layer``` of  ```neural_networks.py```\n",
    "\n",
    "### Q1.3 Activation function - tanh (10 points)\n",
    "Now, you need to implement the activation function tanh. In this part, you need to implement 2 python functions in ```class tanh```. In ```def forward(self, X)```, you need to implement the forward pass and you need to compute the derivative and accordingly implement ```def backward(self, X, grad)```, i.e. the backward pass.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{tanh}:\\hspace{15pt} & h =\\cfrac{2}{1+e^{-2u}}-1\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "You can use the above formula for tanh as a reference.\n",
    "* ```TODO 5```\n",
    "You need to complete ```def forward(self, X)``` in ```class tanh``` of  ```neural_networks.py```\n",
    "* ```TODO 6```\n",
    "You need to complete ```def backward(self, X, grad)``` in ```class tanh``` of  ```neural_networks.py```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Q1.4 Activation function - relu (10 points)\n",
    "You need to implement another activation function called relu. In this part, you need to implement 2 python functions in ```class relu```. In ```def forward(self, X)```, you need to implement the forward pass and you need to compute the derivative and accordingly implement ```def backward(self, X, grad)```, i.e. the backward pass.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{relu}: \\hspace{15pt} & h = max\\{0, u\\} =\n",
    "\\begin{bmatrix}\n",
    "\\max\\{0, u_1\\}\\\\\n",
    "\\vdots \\\\\n",
    "\\max\\{0, u_M\\}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "You can use the above formula for relu as a reference.\n",
    "* ```TODO 7```\n",
    "You need to complete ```def forward(self, X)``` in ```class relu``` of  ```neural_networks.py```\n",
    "* ```TODO 8```\n",
    "You need to complete ```def backward(self, X, grad)``` in ```class relu``` of  ```neural_networks.py```\n",
    "\n",
    "\n",
    "\n",
    "### Q1.5 Dropout (10 points)\n",
    "To prevent overfitting, we usually add regularization. Dropout is another way of handling overfitting. In this part, you will initially read and understand ```def forward(self, X, is_train)``` i.e. the forward pass of ```class dropout``` and derive partial derivatives accordingly to implement ```def backward(self, X, grad)``` i.e. the backward pass of ```class dropout```. We define the forward and the backward passes as follows.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{forward pass:}\\hspace{2em} &\n",
    "{s} = \\text{dropout}\\text{.forward}({q}\\in\\mathbb{R}^J) = \\frac{1}{1-r}\\times\n",
    "\\begin{bmatrix}\n",
    "\\textbf{1}[p_1 >= r] \\times q_1\\\\\n",
    "\\vdots \\\\\n",
    "\\textbf{1}[p_J >= r] \\times q_J\\\\\n",
    "\\end{bmatrix},\n",
    "\\\\\n",
    "\\nonumber\\\\\n",
    "&\\text{where } p_j \\text{ is sampled uniformly from }[0, 1), \\forall j\\in\\{1,\\cdots,J\\}, \\nonumber\\\\\n",
    "&\\text{and } r\\in [0, 1) \\text{ is a pre-defined scalar named dropout rate}.\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "\\text{backward pass:}\\hspace{2em} &\\frac{\\partial l}{\\partial {q}} = \\text{dropout}\\text{.backward}({q}, \\frac{\\partial l}{\\partial {s}})=\n",
    "\\frac{1}{1-r}\\times\n",
    "\\begin{bmatrix}\n",
    "\\textbf{1}[p_1 >= r] \\times \\cfrac{\\partial l}{\\partial s_1}\\\\\n",
    "\\vdots \\\\\n",
    "\\textbf{1}[p_J >= r] \\times \\cfrac{\\partial l}{\\partial s_J}\\\\\n",
    "\\end{bmatrix}.\n",
    "\\end{align}\n",
    "\n",
    "Note that $p_j, j\\in\\{1,\\cdots,J\\}$ and $r$ are not be learned so we do not need to compute the derivatives w.r.t. to them. Moreover, $p_j, j\\in\\{1,\\cdots,J\\}$ are re-sampled every forward pass, and are kept for the following backward pass. The dropout rate $r$ is set to 0 during testing.\n",
    "\n",
    "* ```TODO 9```\n",
    "You need to complete ```def backward(self, X, grad)``` in ```class dropout``` of  ```neural_networks.py```\n",
    "\n",
    "### Q1.6 Connecting the dots\n",
    "In this part, you will combine the modules written from question Q1.1 to Q1.5 by implementing TODO snippets in the ```def main(main_params, optimization_type=\"minibatch_sgd\")``` i.e. main function. After implementing forward and backward passes of MLP layers in Q1.1 to Q1.5,now in the main function you will call the forward methods and backward methods of every layer in the model in an appropriate order based on the architecture.\n",
    "\n",
    "* ```TODO 10```\n",
    "You need to complete ```main(main_params, optimization_type=\"minibatch_sgd\")``` in ```neural_networks.py```\n",
    "\n",
    "\n",
    "\n",
    "### Grading\n",
    "Your code will be graded on Vocareum with autograding script. For your reference, the solution code takes around 5 minutes to execute. As long as your code can finish grading on Vocareum, you should be good. When you finish all ```TODO``` parts, please click submit button on Vocareum. Sometimes you may need to come back to check grading report later.\n",
    "\n",
    "Your code will be tested on the correctness of modules you have implemented as well as certain custom testcases. 40 points are assigned for Question 1 while 60 points are assigned to custom testcases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
